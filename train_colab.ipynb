{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Addition Task Training Notebook\n",
        "\n",
        "This notebook trains a small transformer from scratch to perform k=4 digit addition.\n",
        "\n",
        "## Task Format\n",
        "\n",
        "- Input: `\"1234 + 5678 =\"`\n",
        "- Output: `\"6912\"`\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "- In-distribution (ID): Same format as training\n",
        "- Out-of-distribution (OOD): `\"1 234 + 5 678=\"` → accepts `\"6912\"` or `\"6 912\"` (any spacing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers datasets trl wandb hydra-core matplotlib seaborn pandas\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\n",
        "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive or upload files\n",
        "# Option 1: If repo is in Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/path/to/repo')\n",
        "\n",
        "# Option 2: Upload files directly (run this cell and upload the src/ folder)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Option 3: Clone from GitHub\n",
        "# !git clone https://github.com/yourusername/cbai_tt.git\n",
        "# import sys\n",
        "# sys.path.append('/content/cbai_tt')\n",
        "\n",
        "# For now, we'll assume the repo is available\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.makedirs(\"src\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Task Setup Documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task configuration\n",
        "task_setup = {\n",
        "    \"task\": \"Addition of two k-digit numbers\",\n",
        "    \"k\": 4,\n",
        "    \"input_format\": \"1234 + 5678 =\",\n",
        "    \"output_format\": \"6912\",\n",
        "    \"model_architecture\": {\n",
        "        \"num_layers\": 4,\n",
        "        \"hidden_size\": 128,\n",
        "        \"num_attention_heads\": 4,\n",
        "        \"intermediate_size\": 512,\n",
        "        \"vocab_size\": 13,  # 0-9, +, =, space\n",
        "    },\n",
        "    \"tokenizer\": {\n",
        "        \"type\": \"character-level\",\n",
        "        \"vocab_size\": 13,\n",
        "        \"tokens\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"+\", \"=\", \" \"],\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"Task Setup:\")\n",
        "print(f\"  Task: {task_setup['task']}\")\n",
        "print(f\"  k: {task_setup['k']}\")\n",
        "print(f\"  Format: {task_setup['input_format']} → {task_setup['output_format']}\")\n",
        "print(f\"  Model: {task_setup['model_architecture']}\")\n",
        "print(\n",
        "    f\"  Tokenizer: {task_setup['tokenizer']['type']} with {task_setup['tokenizer']['vocab_size']} tokens\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Generation and Diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "from src.tokenizer import CharTokenizer\n",
        "from src.dataset import FinetuningDataset, get_datasets\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = CharTokenizer()\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
        "print(f\"Sample encode: {tokenizer.encode('1234 + 5678 =')}\")\n",
        "print(f\"Sample decode: {tokenizer.decode([1, 2, 3, 4, 10, 1, 5, 6, 7, 8, 11])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate datasets\n",
        "seed = 1\n",
        "k = 4\n",
        "train_size = 50000\n",
        "val_size = 10000  # Match test size for better balance\n",
        "test_id_size = 10000\n",
        "test_ood_size = 10000\n",
        "\n",
        "dataset = FinetuningDataset(\n",
        "    seed=seed,\n",
        "    tokenizer=tokenizer,\n",
        "    apply_chat_template=False,\n",
        "    k=k,\n",
        "    train_size=train_size,\n",
        "    val_size=val_size,\n",
        "    test_id_size=test_id_size,\n",
        "    test_ood_size=test_ood_size,\n",
        ")\n",
        "\n",
        "train_dataset, val_dataset, test_id_dataset = dataset.generate_data()\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")\n",
        "print(f\"Test ID size: {len(test_id_dataset)}\")\n",
        "\n",
        "# Show samples\n",
        "print(\"\\nSample training examples:\")\n",
        "for i in range(3):\n",
        "    ex = train_dataset[i]\n",
        "    print(f\"  {ex['prompt']} → {ex['completion']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify no overlap between splits\n",
        "train_pairs = {(ex[\"a\"], ex[\"b\"]) for ex in train_dataset}\n",
        "val_pairs = {(ex[\"a\"], ex[\"b\"]) for ex in val_dataset}\n",
        "test_pairs = {(ex[\"a\"], ex[\"b\"]) for ex in test_id_dataset}\n",
        "\n",
        "overlap_train_val = len(train_pairs & val_pairs)\n",
        "overlap_train_test = len(train_pairs & test_pairs)\n",
        "overlap_val_test = len(val_pairs & test_pairs)\n",
        "\n",
        "print(f\"Overlap train-val: {overlap_train_val} (should be 0)\")\n",
        "print(f\"Overlap train-test: {overlap_train_test} (should be 0)\")\n",
        "print(f\"Overlap val-test: {overlap_val_test} (should be 0)\")\n",
        "\n",
        "data_diagnostics = {\n",
        "    \"train_size\": len(train_dataset),\n",
        "    \"val_size\": len(val_dataset),\n",
        "    \"test_id_size\": len(test_id_dataset),\n",
        "    \"overlap_train_val\": overlap_train_val,\n",
        "    \"overlap_train_test\": overlap_train_test,\n",
        "    \"overlap_val_test\": overlap_val_test,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distribution of sums\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_sums = [ex[\"result\"] for ex in train_dataset]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(train_sums, bins=50, edgecolor=\"black\")\n",
        "plt.xlabel(\"Sum (a + b)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Sums in Training Data\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sum_distribution.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Sum range: {min(train_sums)} to {max(train_sums)}\")\n",
        "print(f\"Mean sum: {np.mean(train_sums):.2f}\")\n",
        "print(f\"Std sum: {np.std(train_sums):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "import time\n",
        "import wandb\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# Hyperparameters\n",
        "training_config = {\n",
        "    \"batch_size\": 64,  # Batch size per GPU (64 examples per training step)\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"num_epochs\": 20,  # Reduced to prevent overfitting, will select best epoch later\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"warmup_ratio\": 0.0,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"hidden_size\": 128,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 512,\n",
        "    \"vocab_size\": len(tokenizer),\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Batch size: {training_config['batch_size']}\")\n",
        "print(f\"  Learning rate: {training_config['learning_rate']}\")\n",
        "print(f\"  Epochs: {training_config['num_epochs']}\")\n",
        "print(f\"\\nModel Configuration:\")\n",
        "for k, v in model_config.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Track GPU info\n",
        "if torch.cuda.is_available():\n",
        "    gpu_info = {\n",
        "        \"device\": torch.cuda.get_device_name(0),\n",
        "        \"memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
        "    }\n",
        "    print(f\"\\nGPU: {gpu_info['device']}\")\n",
        "    print(f\"GPU Memory: {gpu_info['memory_gb']:.2f} GB\")\n",
        "else:\n",
        "    gpu_info = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize wandb (optional - comment out if not using)\n",
        "# wandb.login()\n",
        "# wandb.init(\n",
        "#     project=\"addition-task\",\n",
        "#     config={**training_config, **model_config}\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model from scratch\n",
        "config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "config.num_hidden_layers = model_config[\"num_hidden_layers\"]\n",
        "config.hidden_size = model_config[\"hidden_size\"]\n",
        "config.num_attention_heads = model_config[\"num_attention_heads\"]\n",
        "config.intermediate_size = model_config[\"intermediate_size\"]\n",
        "config.vocab_size = model_config[\"vocab_size\"]\n",
        "\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(\n",
        "    f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training arguments\n",
        "output_dir = \"./checkpoints\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "effective_batch_size = (\n",
        "    training_config[\"batch_size\"] * training_config[\"gradient_accumulation_steps\"]\n",
        ")\n",
        "steps_per_epoch = len(train_dataset) // effective_batch_size\n",
        "total_steps = steps_per_epoch * training_config[\"num_epochs\"]\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    report_to=\"wandb\" if wandb.run else None,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=max(1, steps_per_epoch // 10),\n",
        "    num_train_epochs=training_config[\"num_epochs\"],\n",
        "    completion_only_loss=True,\n",
        "    per_device_train_batch_size=training_config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=training_config[\"batch_size\"],\n",
        "    save_steps=max(1, steps_per_epoch // 5),\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=5,\n",
        "    eval_steps=max(1, steps_per_epoch // 5),\n",
        "    eval_strategy=\"steps\",\n",
        "    gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n",
        "    learning_rate=training_config[\"learning_rate\"],\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    weight_decay=training_config[\"weight_decay\"],\n",
        "    warmup_ratio=training_config[\"warmup_ratio\"],\n",
        "    max_grad_norm=training_config[\"max_grad_norm\"],\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
        "    fp16=False,\n",
        "    prediction_loss_only=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        ")\n",
        "\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total steps: {total_steps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Track training time\n",
        "start_time = time.time()\n",
        "print(f\"Starting training at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(\n",
        "    f\"\\nTraining completed in {training_time / 60:.2f} minutes ({training_time:.2f} seconds)\"\n",
        ")\n",
        "\n",
        "# Save model and tokenizer\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model and tokenizer saved to {output_dir}\")\n",
        "\n",
        "training_details = {\n",
        "    **training_config,\n",
        "    **model_config,\n",
        "    \"training_time_seconds\": training_time,\n",
        "    \"training_time_minutes\": training_time / 60,\n",
        "    \"gpu_info\": gpu_info,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training logs\n",
        "import json\n",
        "\n",
        "# Get training history from trainer\n",
        "if hasattr(trainer.state, \"log_history\"):\n",
        "    logs = trainer.state.log_history\n",
        "\n",
        "    # Extract loss and eval loss\n",
        "    train_losses = [\n",
        "        log[\"loss\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log\n",
        "    ]\n",
        "    eval_losses = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
        "    steps = [log[\"step\"] for log in logs if \"loss\" in log or \"eval_loss\" in log]\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if train_losses:\n",
        "        plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\")\n",
        "    if eval_losses:\n",
        "        plt.plot(range(len(eval_losses)), eval_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if train_losses:\n",
        "        plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\")\n",
        "    if eval_losses:\n",
        "        plt.plot(range(len(eval_losses)), eval_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss (log scale)\")\n",
        "    plt.title(\"Training and Validation Loss (Log Scale)\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_curves.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Final train loss: {train_losses[-1] if train_losses else 'N/A'}\")\n",
        "    print(f\"Final val loss: {eval_losses[-1] if eval_losses else 'N/A'}\")\n",
        "else:\n",
        "    print(\"No training logs available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
        "tokenizer = CharTokenizer.from_pretrained(output_dir)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model and tokenizer loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate_dataset(model, tokenizer, dataset, max_new_tokens=8):\n",
        "    \"\"\"Evaluate model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    exact_correct = 0\n",
        "    normalized_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    def normalize_whitespace(text):\n",
        "        return \"\".join(text.split()) if text else \"\"\n",
        "\n",
        "    sample_predictions = []\n",
        "\n",
        "    for i, example in enumerate(dataset):\n",
        "        prompt = example[\"prompt\"]\n",
        "        ground_truth = example[\"completion\"]\n",
        "\n",
        "        # Encode prompt\n",
        "        input_ids = tokenizer.encode(prompt)\n",
        "        input_tensor = torch.tensor([input_ids])\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_tensor,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,  # Greedy decoding\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode only the new tokens\n",
        "        generated_ids = outputs[0][len(input_ids) :]\n",
        "        prediction = tokenizer.decode(generated_ids).strip()\n",
        "\n",
        "        # Evaluate\n",
        "        exact_match = prediction == ground_truth\n",
        "        normalized_match = normalize_whitespace(prediction) == normalize_whitespace(\n",
        "            ground_truth\n",
        "        )\n",
        "\n",
        "        if exact_match:\n",
        "            exact_correct += 1\n",
        "        if normalized_match:\n",
        "            normalized_correct += 1\n",
        "        total += 1\n",
        "\n",
        "        # Store samples\n",
        "        if len(sample_predictions) < 10:\n",
        "            sample_predictions.append(\n",
        "                {\n",
        "                    \"prompt\": prompt,\n",
        "                    \"ground_truth\": ground_truth,\n",
        "                    \"prediction\": prediction,\n",
        "                    \"exact_match\": exact_match,\n",
        "                    \"normalized_match\": normalized_match,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            print(f\"Evaluated {i + 1}/{len(dataset)} examples\")\n",
        "\n",
        "    exact_accuracy = exact_correct / total if total > 0 else 0\n",
        "    normalized_accuracy = normalized_correct / total if total > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"exact_accuracy\": exact_accuracy,\n",
        "        \"normalized_accuracy\": normalized_accuracy,\n",
        "        \"exact_correct\": exact_correct,\n",
        "        \"normalized_correct\": normalized_correct,\n",
        "        \"total\": total,\n",
        "        \"sample_predictions\": sample_predictions,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on ID test set\n",
        "print(\"Evaluating on ID test set...\")\n",
        "id_results = evaluate_dataset(model, tokenizer, test_id_dataset)\n",
        "\n",
        "print(f\"\\nID Test Results:\")\n",
        "print(\n",
        "    f\"  Exact match accuracy: {id_results['exact_accuracy']:.4f} ({id_results['exact_correct']}/{id_results['total']})\"\n",
        ")\n",
        "print(\n",
        "    f\"  Normalized accuracy: {id_results['normalized_accuracy']:.4f} ({id_results['normalized_correct']}/{id_results['total']})\"\n",
        ")\n",
        "\n",
        "print(\"\\nSample ID predictions:\")\n",
        "for i, sample in enumerate(id_results[\"sample_predictions\"][:5]):\n",
        "    print(\n",
        "        f\"  {i + 1}. {sample['prompt']} → GT: '{sample['ground_truth']}', Pred: '{sample['prediction']}' \"\n",
        "    )\n",
        "    print(\n",
        "        f\"     Exact: {sample['exact_match']}, Normalized: {sample['normalized_match']}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and evaluate on OOD test set\n",
        "print(\"Generating OOD test set...\")\n",
        "\n",
        "# Get exclude pairs from ID splits\n",
        "all_id_pairs = train_pairs | val_pairs | test_pairs\n",
        "test_ood_dataset = dataset.generate_ood_data(exclude_pairs=all_id_pairs)\n",
        "\n",
        "print(f\"OOD test size: {len(test_ood_dataset)}\")\n",
        "print(\"\\nSample OOD examples:\")\n",
        "for i in range(3):\n",
        "    ex = test_ood_dataset[i]\n",
        "    print(f\"  {ex['prompt']} → {ex['completion']}\")\n",
        "\n",
        "print(\"\\nEvaluating on OOD test set...\")\n",
        "ood_results = evaluate_dataset(model, tokenizer, test_ood_dataset)\n",
        "\n",
        "print(f\"\\nOOD Test Results:\")\n",
        "print(\n",
        "    f\"  Exact match accuracy: {ood_results['exact_accuracy']:.4f} ({ood_results['exact_correct']}/{ood_results['total']})\"\n",
        ")\n",
        "print(\n",
        "    f\"  Normalized accuracy: {ood_results['normalized_accuracy']:.4f} ({ood_results['normalized_correct']}/{ood_results['total']})\"\n",
        ")\n",
        "\n",
        "print(\"\\nSample OOD predictions:\")\n",
        "for i, sample in enumerate(ood_results[\"sample_predictions\"][:5]):\n",
        "    print(\n",
        "        f\"  {i + 1}. {sample['prompt']} → GT: '{sample['ground_truth']}', Pred: '{sample['prediction']}' \"\n",
        "    )\n",
        "    print(\n",
        "        f\"     Exact: {sample['exact_match']}, Normalized: {sample['normalized_match']}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "results = {\n",
        "    \"id_test\": {\n",
        "        \"exact\": id_results[\"exact_accuracy\"],\n",
        "        \"normalized\": id_results[\"normalized_accuracy\"],\n",
        "        \"exact_correct\": id_results[\"exact_correct\"],\n",
        "        \"normalized_correct\": id_results[\"normalized_correct\"],\n",
        "        \"total\": id_results[\"total\"],\n",
        "    },\n",
        "    \"ood_test\": {\n",
        "        \"exact\": ood_results[\"exact_accuracy\"],\n",
        "        \"normalized\": ood_results[\"normalized_accuracy\"],\n",
        "        \"exact_correct\": ood_results[\"exact_correct\"],\n",
        "        \"normalized_correct\": ood_results[\"normalized_correct\"],\n",
        "        \"total\": ood_results[\"total\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "with open(\"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Visualization and Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results table\n",
        "import pandas as pd\n",
        "\n",
        "results_table = pd.DataFrame(\n",
        "    {\n",
        "        \"Test Set\": [\"ID Test\", \"OOD Test\"],\n",
        "        \"Exact Match\": [results[\"id_test\"][\"exact\"], results[\"ood_test\"][\"exact\"]],\n",
        "        \"Normalized Match\": [\n",
        "            results[\"id_test\"][\"normalized\"],\n",
        "            results[\"ood_test\"][\"normalized\"],\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Results Summary:\")\n",
        "print(results_table.to_string(index=False))\n",
        "\n",
        "# Save as CSV\n",
        "results_table.to_csv(\"results_table.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot accuracy comparison\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "x = [\"ID Test\", \"OOD Test\"]\n",
        "exact_accs = [results[\"id_test\"][\"exact\"], results[\"ood_test\"][\"exact\"]]\n",
        "norm_accs = [results[\"id_test\"][\"normalized\"], results[\"ood_test\"][\"normalized\"]]\n",
        "\n",
        "x_pos = np.arange(len(x))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x_pos - width / 2, exact_accs, width, label=\"Exact Match\", alpha=0.8)\n",
        "ax.bar(x_pos + width / 2, norm_accs, width, label=\"Normalized Match\", alpha=0.8)\n",
        "\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Model Performance: ID vs OOD Test Sets\")\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(x)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "ax.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"accuracy_comparison.png\", dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Report Summary Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile all information for report\n",
        "report_data = {\n",
        "    \"task_setup\": task_setup,\n",
        "    \"data_diagnostics\": data_diagnostics,\n",
        "    \"training_details\": training_details,\n",
        "    \"results\": results,\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"report_data.json\", \"w\") as f:\n",
        "    json.dump(report_data, f, indent=2)\n",
        "\n",
        "print(\"Report data saved to report_data.json\")\n",
        "\n",
        "# Print summary for easy copy-paste\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"REPORT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. Task Setup:\")\n",
        "print(f\"   Format: {task_setup['input_format']} → {task_setup['output_format']}\")\n",
        "print(f\"   k: {task_setup['k']}\")\n",
        "print(f\"   Model: {task_setup['model_architecture']}\")\n",
        "print(\n",
        "    f\"   Tokenizer: {task_setup['tokenizer']['type']} ({task_setup['tokenizer']['vocab_size']} tokens)\"\n",
        ")\n",
        "\n",
        "print(\"\\n2. Data Design:\")\n",
        "print(f\"   Train: {data_diagnostics['train_size']}\")\n",
        "print(f\"   Val: {data_diagnostics['val_size']}\")\n",
        "print(f\"   Test ID: {data_diagnostics['test_id_size']}\")\n",
        "print(\n",
        "    f\"   Overlaps: {data_diagnostics['overlap_train_val']}, {data_diagnostics['overlap_train_test']}, {data_diagnostics['overlap_val_test']}\"\n",
        ")\n",
        "\n",
        "print(\"\\n3. Training Details:\")\n",
        "print(f\"   Batch size: {training_config['batch_size']}\")\n",
        "print(f\"   Learning rate: {training_config['learning_rate']}\")\n",
        "print(f\"   Epochs: {training_config['num_epochs']}\")\n",
        "print(f\"   Training time: {training_details['training_time_minutes']:.2f} minutes\")\n",
        "if gpu_info:\n",
        "    print(f\"   GPU: {gpu_info['device']}\")\n",
        "\n",
        "print(\"\\n4. Results:\")\n",
        "print(\n",
        "    f\"   ID Test - Exact: {results['id_test']['exact']:.4f}, Normalized: {results['id_test']['normalized']:.4f}\"\n",
        ")\n",
        "print(\n",
        "    f\"   OOD Test - Exact: {results['ood_test']['exact']:.4f}, Normalized: {results['ood_test']['normalized']:.4f}\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Interpretation and Discussion\n",
        "\n",
        "Use the cells below to add your interpretation and discussion:\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- What did the model learn?\n",
        "- How well does it generalize?\n",
        "- What patterns do you observe in errors?\n",
        "\n",
        "### Discussion\n",
        "\n",
        "- How did you design the task format and why?\n",
        "- How did you design the train/test split and why?\n",
        "- What remains uncertain?\n",
        "- What would you do next?\n",
        "- Why is this task hard for the model?\n",
        "- What could improve its performance?\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
